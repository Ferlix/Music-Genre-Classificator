{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Baseline_CNN_Music_Genre_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L3ekx2Mt5-hF",
        "-GsmgDD_Qymh",
        "-W6QvaEJ5-hY",
        "oOZRIlOS5-hg",
        "VImQ0asIRB4k",
        "qNLKighkREhN",
        "p1cy5uNOTFiz",
        "AhYsh5R-1ego",
        "fah_q4Reb5qe"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dLx_lpwQJVu",
        "colab_type": "text"
      },
      "source": [
        "# This script implements a simple baseline-CNN model for the Music Genre Classification task.\n",
        "It can be used as a Jupyter- or Colab-Notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "582qG_SHPuRN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Colab-Specific"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl9-iGPMQcB_",
        "colab_type": "text"
      },
      "source": [
        "## Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9QfKlnj8aC2",
        "colab_type": "code",
        "outputId": "8b170ab9-6d7a-45f5-be77-49b156ceeb5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "!pip install rarfile\n",
        "!pip install scikit-image==0.15.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rarfile in /usr/local/lib/python3.6/dist-packages (3.1)\n",
            "Requirement already satisfied: scikit-image==0.15.0 in /usr/local/lib/python3.6/dist-packages (0.15.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.15.0) (1.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.15.0) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.15.0) (6.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.15.0) (3.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.15.0) (2.4)\n",
            "Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.15.0) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /tensorflow-2.1.0/python3.6 (from scipy>=0.17.0->scikit-image==0.15.0) (1.18.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.15.0) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.15.0) (2.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.15.0) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.15.0) (0.10.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image==0.15.0) (4.4.1)\n",
            "Requirement already satisfied: setuptools in /tensorflow-2.1.0/python3.6 (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.15.0) (45.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /tensorflow-2.1.0/python3.6 (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.15.0) (1.13.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TuGXPA8P582",
        "colab_type": "text"
      },
      "source": [
        "## Mount GDrive to Colab session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csZ4ZhEE4pHs",
        "colab_type": "code",
        "outputId": "1b17cf9f-55ff-447f-8122-6034206c7401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFXDtAAOPP2U",
        "colab_type": "text"
      },
      "source": [
        "##  Set path to dataset stored in GDrive\n",
        "Distinguish between train- and test data. Make sure dataset is added to '***My Drive***'! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPL9OSQ5PQp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_storage_path = '/content/drive/My Drive/ML/ML_experiments'\n",
        "path_train_data_set = '/content/drive/My Drive/ML/dataset_transformed/spectrograms512_train'\n",
        "path_test_data_set = '/content/drive/My Drive/ML/dataset_transformed/spectrograms512_test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFGMqYLsbtcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports mainly for debugging\n",
        "import IPython.display as display\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2O4EHPt5-gt",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh8g1UmcTxLV",
        "colab_type": "text"
      },
      "source": [
        "Specific tensorflow setup to make sure it's gonna run on GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrDMIlMSTwN3",
        "colab_type": "code",
        "outputId": "d6e67b3d-de59-4bd9-bb8e-31232949b487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoLuDFDg5-g1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# System/zip-/rar-handling imports\n",
        "import os, sys\n",
        "import zipfile\n",
        "import rarfile # Needs 'unrar'. On Ubuntu: install via \"sudo apt-get install unrar\"; pip didn't work here.\n",
        "\n",
        "# Imports tensorflow\n",
        "from tensorflow.keras import datasets, layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import pathlib\n",
        "\n",
        "# Imports image handling\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage\n",
        "\n",
        "# For generating training and test data\n",
        "import random\n",
        "\n",
        "# Save training progress\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from shutil import copyfile  # Making copy of this file instance (including param settings used)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3ekx2Mt5-hF",
        "colab_type": "text"
      },
      "source": [
        "# IF DEALING WITH ZIP FILE! (Outdated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GsmgDD_Qymh",
        "colab_type": "text"
      },
      "source": [
        "## Get sorted list of training file names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iFPE6s735-hH",
        "colab_type": "code",
        "outputId": "57cb8470-92e4-47a8-c25d-ef79a2e1e3c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Get access to zip-archive\n",
        "# Local:\n",
        "#archive = zipfile.ZipFile('../data/spectrograms.zip', 'r')\n",
        "#imgdata = archive.read('spectrograms/spectrogram_0000.png')\n",
        "\n",
        "# Colab:\n",
        "archive = zipfile.ZipFile('/content/drive/My Drive/ML/data/spectrograms.zip', 'r')\n",
        "imgdata = archive.read('spectrograms/spectrogram_0000.png')\n",
        "\n",
        "files = sorted([f for f in archive.namelist()[1:] if f.startswith('spectrograms/') and f.endswith('.png')])\n",
        "\n",
        "print(files)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3b4cd987a184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/ML/data/spectrograms.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimgdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spectrograms/spectrogram_0000.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spectrograms/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/ML/data/spectrograms.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W6QvaEJ5-hY",
        "colab_type": "text"
      },
      "source": [
        "## Read in both training and testing data from combined zip archive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVnTPkiR5-ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_set = []\n",
        "# Data storage\n",
        "combined_data = np.empty([1, 128, 128])\n",
        "\n",
        "# Read in images & store processed instances\n",
        "for f_name in files:\n",
        "    # Get image data from zip file\n",
        "    zip_img_data = archive.read(f_name)\n",
        "    image = cv2.imdecode(np.frombuffer(zip_img_data, dtype=np.uint8), 1)\n",
        "    \n",
        "    # Normalize image's colors to range [0, 1]\n",
        "    image = image / 255.0\n",
        "\n",
        "    # Grayscale image\n",
        "    gray_image = skimage.color.rgb2gray(image)\n",
        "\n",
        "    # Store grayscaled image\n",
        "    combined_data = np.append(combined_data, [gray_image], axis=0)\n",
        "    \n",
        "# Remove initial, empty datapoint\n",
        "combined_data = combined_data[1:, :, :]\n",
        "\n",
        "print('Done reading in... Shape of data array:')\n",
        "print(combined_data.shape)\n",
        "print('Done.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOZRIlOS5-hg",
        "colab_type": "text"
      },
      "source": [
        "## Read in labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GFyuf9h5-hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_path = '../data/labels.txt'\n",
        "\n",
        "combined_labels = np.empty([1])\n",
        "\n",
        "with open(labels_path, 'r') as file:\n",
        "    for line in file:\n",
        "        combined_labels = np.append(combined_labels, [int(line)])\n",
        "\n",
        "# Remove initial, empty datapoint\n",
        "combined_labels = combined_labels[1:] \n",
        "print(combined_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHwdSTzZ5-ho",
        "colab_type": "text"
      },
      "source": [
        "## Divide data into train and test data:\n",
        "\n",
        "------------------------------------------------------\n",
        "Training data will be contained in:    training_data\n",
        "Tetsing  data will be contained in:    testing_data\n",
        "\n",
        "Training labels will be contained in:  training_labels\n",
        "Tetsing  labels will be contained in:  testing_labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEVZ0w1S5-hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get set of test-indices which indicates the training data points that have to be reserved for training\n",
        "percentage_test_data = 0.2\n",
        "population = range(len(combined_labels))\n",
        "nr_samples = int(percentage_test_data * len(combined_labels))\n",
        "\n",
        "test_indices = random.sample(population, nr_samples)\n",
        "test_indices = sorted(test_indices)\n",
        "\n",
        "print('Nr. test indices: ' + str(len(test_indices)))\n",
        "print('Test indices: ' + str(test_indices))\n",
        "\n",
        "\n",
        "# Split data into training- and test data, respectively - Preparation: Create empty arrays in which to later insert data\n",
        "test_len = len(test_indices)\n",
        "train_len = len(combined_labels)-len(test_indices)\n",
        "training_data, training_labels = np.empty([train_len, 128, 128]), np.empty([train_len])\n",
        "testing_data, testing_labels = np.empty([test_len, 128, 128]), np.empty([test_len])\n",
        "\n",
        "test_idx_list_idx = 0\n",
        "i = 0\n",
        "\n",
        "# Iterate through all data and assign each data point either to training data or testing data\n",
        "for data_idx in range(len(combined_labels)):\n",
        "\n",
        "    if test_idx_list_idx < nr_samples and data_idx == test_indices[test_idx_list_idx]:\n",
        "        testing_data[test_idx_list_idx, :, :] = combined_data[data_idx, :, :]\n",
        "        testing_labels[test_idx_list_idx] = combined_labels[data_idx]\n",
        "        test_idx_list_idx += 1\n",
        "        \n",
        "    else:\n",
        "        \n",
        "        training_data[i, :, :] = combined_data[data_idx, :, :]\n",
        "        training_labels[i] = combined_labels[data_idx]\n",
        "        i += 1\n",
        "        \n",
        "\n",
        "training_data = training_data.reshape([len(training_labels), 128, 128, 1])\n",
        "\n",
        "testing_data = testing_data.reshape([len(testing_labels), 128, 128, 1])\n",
        "        \n",
        "print('Final:')\n",
        "print(training_data.shape)\n",
        "print(training_labels.shape)\n",
        "print(testing_data.shape)\n",
        "print(testing_labels.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VImQ0asIRB4k",
        "colab_type": "text"
      },
      "source": [
        "# IF DEALING WITH RAR FILE *AND* RAM IS LARGE ENOUGH FOR ENTIRE DATA SET! (Outdated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNLKighkREhN",
        "colab_type": "text"
      },
      "source": [
        "## Read in data & create training- and testing data sets (Outdated)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2vV9myL5-h4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = '/content/drive/My Drive/ML/data/'\n",
        "archive_name = 'spectograms_512.rar'\n",
        "\n",
        "if rarfile.is_rarfile(data_path + archive_name):\n",
        "    print('Is rar...')\n",
        "    # Read in file names per data (train vs test) set\n",
        "    class_labels = {'blues': 0,\n",
        "                    'classical': 1,\n",
        "                    'country': 2,\n",
        "                    'disco': 3,\n",
        "                    'hiphop': 4,\n",
        "                    'jazz': 5,\n",
        "                    'metal': 6,\n",
        "                    'pop': 7,\n",
        "                    'reggae': 8,\n",
        "                    'rock': 9\n",
        "                    }\n",
        "\n",
        "    rf = rarfile.RarFile(data_path + archive_name)\n",
        "\n",
        "    train_files, test_files = [], []\n",
        "    train_labels, test_labels = [], []\n",
        "\n",
        "    for f in rf.infolist():\n",
        "        f_name = f.filename\n",
        "        if f_name.startswith('spectrograms') and f_name.endswith('.png'):\n",
        "            p_parts = f_name.split('/')\n",
        "            label = class_labels[p_parts[1]]\n",
        "            if 'train' in f_name:\n",
        "                train_files.append(f_name)\n",
        "                train_labels.append(label)\n",
        "            elif 'test' in f.filename:\n",
        "                test_files.append(f_name)\n",
        "                test_labels.append(label)\n",
        "\n",
        "    num_training_data = len(train_labels)\n",
        "    num_testing_data = len(test_labels)\n",
        "    print('Len train data: ' + str(num_training_data))\n",
        "    print('Len test data: ' + str(num_testing_data))\n",
        "\n",
        "    # Read in data\n",
        "else:\n",
        "    print('No rar')\n",
        "    raise Exception('No rar!')\n",
        "\n",
        "##### TRAINING DATA AND LABELS #####\n",
        "\n",
        "# Get TRAIN image data from rar file\n",
        "dimension = np.array((cv2.imdecode(np.frombuffer(rf.read(train_files[0]), dtype=np.uint8), 1))).shape\n",
        "print('Dimensions: ' + str(dimension))\n",
        "print('Training set size: ' + str(num_training_data))\n",
        "\n",
        "training_data = np.empty([num_training_data, dimension[0], dimension[1]])\n",
        "print('Allocated.')\n",
        "\n",
        "for i, file in enumerate(train_files):\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print('Train: ' + str(i))\n",
        "\n",
        "    try:\n",
        "      rar_img_data = rf.read(file)\n",
        "      image = cv2.imdecode(np.frombuffer(rar_img_data, dtype=np.uint8), 1)\n",
        "\n",
        "      # Normalize image's colors to range [0, 1]\n",
        "      image = image / 255.0\n",
        "\n",
        "      # Grayscale image\n",
        "      gray_image = skimage.color.rgb2gray(image)\n",
        "\n",
        "      # Store grayscaled image\n",
        "      #training_data = np.append(training_data, [gray_image], axis=0)\n",
        "      training_data[i,:,:] = gray_image\n",
        "    except Exception as e:\n",
        "      print('Error: ' + str(file))\n",
        "      print(e)\n",
        "\n",
        "print('Done reading in... Shape of data array:')\n",
        "print(training_data.shape)\n",
        "print('Done.')\n",
        "\n",
        "# Get TRAIN labels\n",
        "training_labels = np.array(train_labels)\n",
        "print(train_labels)\n",
        "\n",
        "\n",
        "##### TESTING DATA AND LABELS #####\n",
        "\n",
        "# Get TEST image data from rar file\n",
        "testing_data = np.empty([num_testing_data, dimension[0], dimension[1]])  # TODO: change procedure here as well to pre-allocating space and NOT removing first element\n",
        "\n",
        "for i, file in enumerate(test_files):\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        print('Train: ' + str(i))\n",
        "    try:\n",
        "      rar_img_data = rf.read(file)\n",
        "      image = cv2.imdecode(np.frombuffer(rar_img_data, dtype=np.uint8), 1)\n",
        "\n",
        "      # Normalize image's colors to range [0, 1]\n",
        "      image = image / 255.0\n",
        "\n",
        "      # Grayscale image\n",
        "      gray_image = skimage.color.rgb2gray(image)\n",
        "\n",
        "      # Store grayscaled image\n",
        "      testing_data[i,:,:] = gray_image\n",
        "    except Exception as e:\n",
        "      print('Error: ' + str(file))\n",
        "      print(e)\n",
        "\n",
        "print('Done reading in... Shape of data array:')\n",
        "print(testing_data.shape)\n",
        "print('Done.')\n",
        "\n",
        "# Get TEST labels\n",
        "testing_labels = np.array(test_labels)\n",
        "print(testing_labels)\n",
        "rf = None # Clear ram space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjCk6BALRQMp",
        "colab_type": "text"
      },
      "source": [
        "# Set up folder for data gathering during training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP6g6QMR5-iC",
        "colab_type": "code",
        "outputId": "526befa7-44de-4df9-9de0-f33651deebf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Set up folder for data gathering during training process\n",
        "now = datetime.now()\n",
        "TIME_STAMP = now.strftime(\"_%Y_%d_%m__%H_%M_%S__%f\")\n",
        "MODEL_ID = 'Model_' + TIME_STAMP\n",
        "training_path = data_storage_path + '/Trained_Models/CNN_Models/'\n",
        "path = training_path + MODEL_ID + '/'\n",
        "\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "    print('Created dir: ' + path)\n",
        "else:\n",
        "    path = None\n",
        "    raise Exception('PATH EXISTS!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created dir: /content/drive/My Drive/ML/ML_experiments/Trained_Models/CNN_Models/Model__2020_22_01__01_39_26__230556/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdqLFsMt5-iG",
        "colab_type": "text"
      },
      "source": [
        "# Set up the CNN architecture & helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1cy5uNOTFiz",
        "colab_type": "text"
      },
      "source": [
        "## Define Model Architecture - Small"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHtomNP-5-iH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%writefile \"\"/content/drive/My Drive/ML/ML_experiments/Trained_Models/CNN_ModelsModel__2020_19_01__09_41_59__410624/model_settings.text\n",
        "#%%writefile $path/model_settings.text \n",
        "# Line above: Save model settings to file for reproducability - Run once with command above and once without. \n",
        "# First, it saves cell's content, but doesn't run the cell, afterwards, it's running the cell\n",
        "\n",
        "# Reset tf sessions\n",
        "tf.keras.backend.clear_session()  # Destroys the current TF graph and creates a new one.\n",
        "\n",
        "dimensions = 512 # Image-Dimension: 512x512x1 (BW-image)\n",
        "classes = 10\n",
        "\n",
        "# Set up model architecture in terms of its layers\n",
        "model = models.Sequential()\n",
        "\n",
        "# Set layers\n",
        "model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(dimensions, dimensions, 1), # 32 batch size\n",
        "                                                       kernel_regularizer=regularizers.l2(0.000000001)\n",
        "          ))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.07))\n",
        "\n",
        "model.add(layers.Conv2D(32, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(0.000000001)\n",
        "          ))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.05))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.0000001)\n",
        "          ))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.12))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0001)\n",
        "          ))\n",
        "\n",
        "model.add(layers.Dropout(0.1)) # KEEP LOW!\n",
        "\n",
        "model.add(layers.Dense(classes, activation='softmax'))\n",
        "\n",
        "# Note on regularizer(s), copied from https://www.tensorflow.org/tutorials/keras/overfit_and_underfit:\n",
        "# l2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value**2\n",
        "# to the total loss of the network.\n",
        "\n",
        "# Print summary\n",
        "model.summary()\n",
        "\n",
        "# Compile model & make some design choices\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001,\n",
        "                                           beta_1=0.9,\n",
        "                                           beta_2=0.999,\n",
        "                                           epsilon=1e-07,\n",
        "                                           amsgrad=False,\n",
        "                                           name='Adam'\n",
        "                                           ),\n",
        "              loss='sparse_categorical_crossentropy',  # Capable of working with regularization\n",
        "              metrics=['accuracy', 'sparse_categorical_crossentropy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvCeDuJrfx53",
        "colab_type": "text"
      },
      "source": [
        "## Define Model Architecture - Large"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-vhPEFR1XFX",
        "colab_type": "text"
      },
      "source": [
        "## Works - Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqX4nddCJxDT",
        "colab_type": "text"
      },
      "source": [
        "1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYIab0yY1cPG",
        "colab_type": "code",
        "outputId": "52cb91c1-ed6d-462a-b9bf-3eb4080ecb5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "#%%writefile \"\"/content/drive/My Drive/ML/ML_experiments/Trained_Models/CNN_ModelsModel__2020_19_01__09_41_59__410624/model_settings.text\n",
        "#%%writefile $path/model_settings.text \n",
        "# Line above: Save model settings to file for reproducability - Run once with command above and once without. \n",
        "# First, it saves cell's content, but doesn't run the cell, afterwards, it's running the cell\n",
        "\n",
        "# Reset tf sessions\n",
        "tf.keras.backend.clear_session()  # Destroys the current TF graph and creates a new one.\n",
        "\n",
        "dimensions = 512 # Image-Dimension: 512x512x1 (BW-image)\n",
        "classes = 10\n",
        "\n",
        "# Set up model architecture in terms of its layers\n",
        "model = models.Sequential()\n",
        "\n",
        "# Set layers\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(dimensions, dimensions, 1), # 32 batch size\n",
        "                                                       #kernel_regularizer=regularizers.l2(0.000000001)\n",
        "          ))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.05))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', #kernel_regularizer=regularizers.l2(0.000000001)  \n",
        "          ))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Dropout(0.05))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', #kernel_regularizer=regularizers.l2(0.000000001)\n",
        "          ))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Dropout(0.05))\n",
        "\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', #kernel_regularizer=regularizers.l2(0.000000001)\n",
        "          ))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Dropout(0.05))\n",
        "\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0001)\n",
        "          ))\n",
        "\n",
        "#model.add(layers.Dropout(0.1)) # KEEP LOW!\n",
        "\n",
        "model.add(layers.Dense(classes, activation='softmax'))\n",
        "\n",
        "# Note on regularizer(s), copied from https://www.tensorflow.org/tutorials/keras/overfit_and_underfit:\n",
        "# l2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value**2\n",
        "# to the total loss of the network.\n",
        "\n",
        "# Print summary\n",
        "model.summary()\n",
        "\n",
        "# Compile model & make some design choices\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001,\n",
        "                                           beta_1=0.9,\n",
        "                                           beta_2=0.999,\n",
        "                                           epsilon=1e-07,\n",
        "                                           amsgrad=False,\n",
        "                                           name='Adagrad'\n",
        "                                           ),\n",
        "              loss='sparse_categorical_crossentropy',  # Capable of working with regularization\n",
        "              metrics=['accuracy', 'sparse_categorical_crossentropy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 510, 510, 64)      640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 255, 255, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 255, 255, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 253, 253, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 126, 126, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 124, 124, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 62, 62, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 60, 60, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 57600)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               7372928   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 7,485,642\n",
            "Trainable params: 7,485,642\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8R_zX1kJzel",
        "colab_type": "text"
      },
      "source": [
        "2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FH_gbxNRJUo_",
        "outputId": "a329e7d4-5038-479e-8f87-a1810f81ffee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "#%%writefile \"\"/content/drive/My Drive/ML/ML_experiments/Trained_Models/CNN_ModelsModel__2020_19_01__09_41_59__410624/model_settings.text\n",
        "#%%writefile $path/model_settings.text \n",
        "# Line above: Save model settings to file for reproducability - Run once with command above and once without. \n",
        "# First, it saves cell's content, but doesn't run the cell, afterwards, it's running the cell\n",
        "\n",
        "# Reset tf sessions\n",
        "tf.keras.backend.clear_session()  # Destroys the current TF graph and creates a new one.\n",
        "\n",
        "dimensions = 512 # Image-Dimension: 512x512x1 (BW-image)\n",
        "classes = 10\n",
        "\n",
        "# Set up model architecture in terms of its layers\n",
        "model = models.Sequential()\n",
        "\n",
        "# Set layers\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', strides=2, input_shape=(dimensions, dimensions, 1), # 32 batch size\n",
        "                                                       kernel_regularizer=regularizers.l2(0.000001)\n",
        "          ))\n",
        "#model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Dropout(0.05))\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.000001)  \n",
        "          ))\n",
        "#model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Dropout(0.05))\n",
        "\n",
        "# changed from 24 to 4\n",
        "model.add(layers.Conv2D(4, (3, 3), activation='relu', strides=2, kernel_regularizer=regularizers.l2(0.000001)\n",
        "          ))\n",
        "#model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Dropout(0.05))\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', strides=2, kernel_regularizer=regularizers.l2(0.000001)\n",
        "          ))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Dropout(0.05))\n",
        "\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', strides=2, kernel_regularizer=regularizers.l2(0.000001)\n",
        "          ))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Dropout(0.05))\n",
        "\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0001)\n",
        "          ))\n",
        "\n",
        "#model.add(layers.Dropout(0.1)) # KEEP LOW!\n",
        "\n",
        "model.add(layers.Dense(classes, activation='softmax'))\n",
        "\n",
        "# Note on regularizer(s), copied from https://www.tensorflow.org/tutorials/keras/overfit_and_underfit:\n",
        "# l2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value**2\n",
        "# to the total loss of the network.\n",
        "\n",
        "# Print summary\n",
        "model.summary()\n",
        "\n",
        "# Compile model & make some design choices\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001,\n",
        "                                           beta_1=0.9,\n",
        "                                           beta_2=0.999,\n",
        "                                           epsilon=1e-07,\n",
        "                                           amsgrad=False,\n",
        "                                           name='Adagrad'\n",
        "                                           ),\n",
        "              loss='sparse_categorical_crossentropy',  # Capable of working with regularization\n",
        "              metrics=['accuracy', 'sparse_categorical_crossentropy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 254, 254, 32)      832       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 252, 252, 32)      9248      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 125, 125, 4)       1156      \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 62, 62, 32)        1184      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               401536    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 433,742\n",
            "Trainable params: 433,742\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mllVL4PJ1li",
        "colab_type": "text"
      },
      "source": [
        "3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-O3V395FurT",
        "colab_type": "code",
        "outputId": "8e6c1040-3e2a-4d25-dc49-6e8086071555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "#%%writefile \"\"/content/drive/My Drive/ML/ML_experiments/Trained_Models/CNN_ModelsModel__2020_19_01__09_41_59__410624/model_settings.text\n",
        "#%%writefile $path/model_settings.text \n",
        "# Line above: Save model settings to file for reproducability - Run once with command above and once without. \n",
        "# First, it saves cell's content, but doesn't run the cell, afterwards, it's running the cell\n",
        "\n",
        "# Reset tf sessions\n",
        "tf.keras.backend.clear_session()  # Destroys the current TF graph and creates a new one.\n",
        "\n",
        "dimensions = 512 # Image-Dimension: 512x512x1 (BW-image)\n",
        "classes = 10\n",
        "\n",
        "# Set up model architecture in terms of its layers\n",
        "model = models.Sequential()\n",
        "\n",
        "# Set layers\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', strides=2, input_shape=(dimensions, dimensions, 1), # 32 batch size\n",
        "                                                       kernel_regularizer=regularizers.l2(0.000000001)\n",
        "          ))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', strides=2, #kernel_regularizer=regularizers.l2(0.000000001)  \n",
        "          ))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', strides=2, #kernel_regularizer=regularizers.l2(0.00000001)\n",
        "          ))\n",
        "model.add(layers.Conv2D(4, (3, 3), activation='relu', strides=2, kernel_regularizer=regularizers.l2(0.001)\n",
        "          ))\n",
        "model.add(layers.Conv2D(4, (3, 3), activation='relu', strides=2, kernel_regularizer=regularizers.l2(0.00001)\n",
        "          ))\n",
        "model.add(layers.Conv2D(8, (3, 3), activation='relu', strides=2, kernel_regularizer=regularizers.l2(0.000000001)\n",
        "          ))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.000001)\n",
        "          ))\n",
        "model.add(layers.Dense(classes, activation='softmax'))\n",
        "\n",
        "# Note on regularizer(s), copied from https://www.tensorflow.org/tutorials/keras/overfit_and_underfit:\n",
        "# l2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value**2\n",
        "# to the total loss of the network.\n",
        "#model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Dropout(0.05))\n",
        "\n",
        "# Print summary\n",
        "model.summary()\n",
        "\n",
        "# Compile model & make some design choices\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001,\n",
        "                                           beta_1=0.9,\n",
        "                                           beta_2=0.999,\n",
        "                                           epsilon=1e-07,\n",
        "                                           amsgrad=False,\n",
        "                                           name='Adam'\n",
        "                                           ),\n",
        "              loss='sparse_categorical_crossentropy',  # Capable of working with regularization\n",
        "              metrics=['accuracy', 'sparse_categorical_crossentropy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 255, 255, 32)      320       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 127, 127, 32)      9248      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 63, 63, 32)        9248      \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 31, 31, 4)         1156      \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 15, 15, 4)         148       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 7, 7, 8)           296       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 392)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                25152     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 46,218\n",
            "Trainable params: 46,218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhYsh5R-1ego",
        "colab_type": "text"
      },
      "source": [
        "## In Progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1ZY5fzHf7-1",
        "colab_type": "code",
        "outputId": "f83ecd09-34e7-4c5a-cc49-ddcab7ac93a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "#%%writefile \"\"/content/drive/My Drive/ML/ML_experiments/Trained_Models/CNN_ModelsModel__2020_19_01__09_41_59__410624/model_settings.text\n",
        "#%%writefile $path/model_settings.text \n",
        "# Line above: Save model settings to file for reproducability - Run once with command above and once without. \n",
        "# First, it saves cell's content, but doesn't run the cell, afterwards, it's running the cell\n",
        " \n",
        "# Reset tf sessions\n",
        "tf.keras.backend.clear_session()  # Destroys the current TF graph and creates a new one.\n",
        " \n",
        "dimensions = 512 # Image-Dimension: 512x512x1 (BW-image)\n",
        "classes = 10\n",
        " \n",
        "# Set up model architecture in terms of its layers\n",
        "model = models.Sequential()\n",
        " \n",
        "# Set layers\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', strides=1, input_shape=(dimensions, dimensions, 1), # 32 batch size\n",
        "                                                       kernel_regularizer=regularizers.l2(0.001)\n",
        "          ))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', strides=2, kernel_regularizer=regularizers.l2(0.001)  \n",
        "          ))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', strides=2, kernel_regularizer=regularizers.l2(0.001)\n",
        "          ))\n",
        "model.add(layers.Conv2D(4, (5, 5), activation='relu', strides=3, kernel_regularizer=regularizers.l2(0.001)\n",
        "          ))\n",
        "model.add(layers.Conv2D(4, (3, 3), activation='relu', strides=2, kernel_regularizer=regularizers.l2(0.001)\n",
        "          ))\n",
        "model.add(layers.Conv2D(8, (3, 3), activation='relu', strides=2, kernel_regularizer=regularizers.l2(0.001)\n",
        "          ))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0000001)\n",
        "          ))\n",
        "model.add(layers.Dense(classes, activation='softmax'))\n",
        " \n",
        "# Note on regularizer(s), copied from https://www.tensorflow.org/tutorials/keras/overfit_and_underfit:\n",
        "# l2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value**2\n",
        "# to the total loss of the network.\n",
        "#model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Dropout(0.05))\n",
        " \n",
        "# Print summary\n",
        "model.summary()\n",
        " \n",
        "# Compile model & make some design choices\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001,\n",
        "                                           beta_1=0.9,\n",
        "                                           beta_2=0.999,\n",
        "                                           epsilon=1e-07,\n",
        "                                           amsgrad=False,\n",
        "                                           name='Adam'\n",
        "                                           ),\n",
        "              loss='sparse_categorical_crossentropy',  # Capable of working with regularization\n",
        "              metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 510, 510, 64)      640       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 254, 254, 64)      36928     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 126, 126, 32)      18464     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 41, 41, 4)         3204      \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 20, 20, 4)         148       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 8)           296       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 648)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               83072     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 144,042\n",
            "Trainable params: 144,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3n-A7sDSV31",
        "colab_type": "text"
      },
      "source": [
        "## Define Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6okhPmWGSX4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definition of callbacks adjusted from https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "        monitor='val_accuracy',    # Stop training when `val_loss` is no longer improving\n",
        "        min_delta=0,               # \"no longer improving\" being defined as \"no better than 0|5e-1 less\"\n",
        "        patience=2,                # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
        "        verbose=0)                 # Quantity of printed output\n",
        "\n",
        "model_saving_callback = ModelCheckpoint(\n",
        "        filepath=path+'cnn_model.h5',\n",
        "        # Path where to save the model\n",
        "        # The two parameters below mean that we will overwrite\n",
        "        # the current checkpoint if and only if\n",
        "        # the `val_loss` score has improved.\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        # mode: one of {auto, min, max}. If `save_best_only=True`, the decision to\n",
        "        # overwrite the current save file is made based on either the maximization\n",
        "        # or the minimization of the monitored quantity. For `val_acc`, this\n",
        "        # should be `max`, for `val_loss` this should be `min`, etc. In `auto`\n",
        "        # mode, the direction is automatically inferred from the name of the\n",
        "        # monitored quantity.\n",
        "        verbose=0)\n",
        "\n",
        "# Join list of required callbacks\n",
        "callbacks = [model_saving_callback] # Outtake: early_stopping_callback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVkNbkeiS2hE",
        "colab_type": "text"
      },
      "source": [
        "## Define Data Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qokIatTdkF3Y",
        "colab_type": "text"
      },
      "source": [
        "Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ2Op1NQkJAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing_function(x):\n",
        "    \"\"\"\n",
        "    Could be used for rotating image in RNN, for example.\n",
        "    \"\"\"\n",
        "    #assert x.shape == (512, 512, 1)\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myfDYhM6ZpM8",
        "colab_type": "text"
      },
      "source": [
        "## Training data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRRmima-S7T7",
        "colab_type": "code",
        "outputId": "4e955961-5b5d-43e2-e1be-8fa5590a2a44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The 1./255 is to convert from uint8 to float32 in range [0,1].\n",
        "train_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, \n",
        "                                                     #preprocessing_function=preprocessing_function\n",
        "                                                     )\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_HEIGHT = 512\n",
        "IMG_WIDTH = 512\n",
        "STEPS_PER_EPOCH = 25 #np.ceil(image_count/BATCH_SIZE)\n",
        "data_dir = path_train_data_set\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "\n",
        "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
        "\n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(directory=str(data_dir),\n",
        "                                                     batch_size=BATCH_SIZE,\n",
        "                                                     color_mode='grayscale',\n",
        "                                                     class_mode='sparse', # Class represented by 1 integer (instead of categorical==1-hot-encoding)\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     classes = list(CLASS_NAMES))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9600 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lJZf5UvcOis",
        "colab_type": "text"
      },
      "source": [
        "## Test|Evaluation data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7B-5G9UcSL1",
        "colab_type": "code",
        "outputId": "aaa50f7b-d268-4d67-e28e-a5a81ba3ec7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The 1./255 is to convert from uint8 to float32 in range [0,1].\n",
        "test_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, \n",
        "                                                     #preprocessing_function=preprocessing_function\n",
        "                                                     )\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_HEIGHT = 512\n",
        "IMG_WIDTH = 512\n",
        "STEPS_PER_EPOCH = 25 #np.ceil(image_count/BATCH_SIZE)\n",
        "data_dir = path_test_data_set\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "\n",
        "\n",
        "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
        "\n",
        "\n",
        "test_data_gen = test_image_generator.flow_from_directory(directory=str(data_dir),\n",
        "                                                     batch_size=BATCH_SIZE,\n",
        "                                                     color_mode='grayscale',\n",
        "                                                     class_mode='sparse',\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     classes = list(CLASS_NAMES))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 600 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fah_q4Reb5qe",
        "colab_type": "text"
      },
      "source": [
        "# Test data imports  (train) generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBY7H4l8cEZz",
        "colab_type": "text"
      },
      "source": [
        "### Helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSpuLmVwaV7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_batch(image_batch, label_batch):\n",
        "  print('Batch dimensions: ' + str(image_batch.shape))\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for n in range(25):\n",
        "      ax = plt.subplot(5,5,n+1)\n",
        "      image = image_batch[n]\n",
        "      # If grayscale, repeat 1 channel 3 times for visualization\n",
        "      if image.shape[2] == 1:\n",
        "        image = np.repeat(image, 3, axis=2)\n",
        "      plt.imshow(image)\n",
        "      plt.title(CLASS_NAMES[label_batch[n]==1][0].title())  # Doesn't work anymore after change of label encoding from 1-hot to spare\n",
        "      plt.axis('off')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmpqlR-ib_6n",
        "colab_type": "text"
      },
      "source": [
        "### Visualize generated example batch and labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n42lwOikbLjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_batch, label_batch = next(train_data_gen)\n",
        "show_batch(image_batch, label_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeOQv1XzSn3S",
        "colab_type": "text"
      },
      "source": [
        "# Perform training\n",
        "\n",
        "'accuracy' == accuracy achieved during training on training data\n",
        "\n",
        "'val_accuracy' == accuracy achieved on Test/Evaluation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkcIB61Q5-iO",
        "colab_type": "code",
        "outputId": "feca757b-3805-4175-9b64-09f289aaf7eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "assert tf.test.is_gpu_available()\n",
        "assert tf.test.is_built_with_cuda()\n",
        "\n",
        "# Set number of desired epochs (mind the early-stopping!)\n",
        "epochs = 100\n",
        "\n",
        "# Perform x epochs of training\n",
        "#history = model.fit(training_data, training_labels,\n",
        "#                    epochs=epochs,\n",
        "#                    validation_data=(testing_data, testing_labels),\n",
        "#                    callbacks=callbacks, verbose=1)\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  history = model.fit_generator(\n",
        "                                train_data_gen,\n",
        "                                steps_per_epoch=25, \n",
        "                                epochs=epochs,\n",
        "                                validation_data=test_data_gen,\n",
        "                                validation_steps=18,\n",
        "                                callbacks=callbacks, \n",
        "                                max_queue_size=2,\n",
        "                                verbose=1\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# Save the entire model as a final model to a HDF5 file.\n",
        "name = 'final_model'\n",
        "model.save(path+name+'.h5')\n",
        "\n",
        "# Record training progress\n",
        "with open(path+'training_progress.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"epoch\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\", \"sparse_categorical_crossentropy\"])\n",
        "    for line in range(len(history.history['loss'])): \n",
        "        epoch = str(line+1)\n",
        "        writer.writerow([epoch,\n",
        "                         history.history[\"loss\"][line], \n",
        "                         history.history[\"accuracy\"][line], \n",
        "                         history.history[\"val_loss\"][line], \n",
        "                         history.history[\"val_accuracy\"][line], \n",
        "                         history.history[\"sparse_categorical_crossentropy\"][line]\n",
        "                         ])\n",
        "    file.close()\n",
        "\n",
        "print('Done.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 25 steps, validate for 18 steps\n",
            "Epoch 1/100\n",
            "25/25 [==============================] - 9s 360ms/step - loss: 2.4521 - accuracy: 0.0962 - sparse_categorical_crossentropy: 2.4320 - val_loss: 2.3259 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3061\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - 8s 319ms/step - loss: 2.3238 - accuracy: 0.0913 - sparse_categorical_crossentropy: 2.3050 - val_loss: 2.3201 - val_accuracy: 0.0972 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - 8s 323ms/step - loss: 2.3189 - accuracy: 0.0938 - sparse_categorical_crossentropy: 2.3027 - val_loss: 2.3175 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - 8s 318ms/step - loss: 2.3158 - accuracy: 0.1050 - sparse_categorical_crossentropy: 2.3020 - val_loss: 2.3156 - val_accuracy: 0.0990 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3157 - accuracy: 0.0925 - sparse_categorical_crossentropy: 2.3035 - val_loss: 2.3140 - val_accuracy: 0.0990 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - 8s 321ms/step - loss: 2.3140 - accuracy: 0.0775 - sparse_categorical_crossentropy: 2.3032 - val_loss: 2.3128 - val_accuracy: 0.0972 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - 8s 319ms/step - loss: 2.3118 - accuracy: 0.1063 - sparse_categorical_crossentropy: 2.3021 - val_loss: 2.3118 - val_accuracy: 0.0990 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - 8s 319ms/step - loss: 2.3110 - accuracy: 0.1100 - sparse_categorical_crossentropy: 2.3022 - val_loss: 2.3111 - val_accuracy: 0.0990 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - 8s 318ms/step - loss: 2.3112 - accuracy: 0.1000 - sparse_categorical_crossentropy: 2.3032 - val_loss: 2.3104 - val_accuracy: 0.0990 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3099 - accuracy: 0.1037 - sparse_categorical_crossentropy: 2.3025 - val_loss: 2.3099 - val_accuracy: 0.0990 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - 8s 319ms/step - loss: 2.3099 - accuracy: 0.0938 - sparse_categorical_crossentropy: 2.3031 - val_loss: 2.3094 - val_accuracy: 0.0990 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3090 - accuracy: 0.0925 - sparse_categorical_crossentropy: 2.3027 - val_loss: 2.3089 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - 8s 318ms/step - loss: 2.3088 - accuracy: 0.0875 - sparse_categorical_crossentropy: 2.3029 - val_loss: 2.3085 - val_accuracy: 0.0990 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - 8s 329ms/step - loss: 2.3077 - accuracy: 0.0875 - sparse_categorical_crossentropy: 2.3022 - val_loss: 2.3082 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - 8s 327ms/step - loss: 2.3064 - accuracy: 0.1475 - sparse_categorical_crossentropy: 2.3013 - val_loss: 2.3080 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3030\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - 8s 318ms/step - loss: 2.3077 - accuracy: 0.0962 - sparse_categorical_crossentropy: 2.3029 - val_loss: 2.3077 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3030\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - 8s 326ms/step - loss: 2.3075 - accuracy: 0.1037 - sparse_categorical_crossentropy: 2.3029 - val_loss: 2.3074 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3030\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - 8s 323ms/step - loss: 2.3073 - accuracy: 0.1063 - sparse_categorical_crossentropy: 2.3030 - val_loss: 2.3071 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3059 - accuracy: 0.1213 - sparse_categorical_crossentropy: 2.3019 - val_loss: 2.3069 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3030\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - 8s 318ms/step - loss: 2.3073 - accuracy: 0.0975 - sparse_categorical_crossentropy: 2.3035 - val_loss: 2.3066 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3074 - accuracy: 0.0825 - sparse_categorical_crossentropy: 2.3038 - val_loss: 2.3063 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3058 - accuracy: 0.1088 - sparse_categorical_crossentropy: 2.3024 - val_loss: 2.3060 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - 8s 319ms/step - loss: 2.3053 - accuracy: 0.1112 - sparse_categorical_crossentropy: 2.3020 - val_loss: 2.3059 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3056 - accuracy: 0.1037 - sparse_categorical_crossentropy: 2.3025 - val_loss: 2.3057 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - 8s 319ms/step - loss: 2.3066 - accuracy: 0.1000 - sparse_categorical_crossentropy: 2.3037 - val_loss: 2.3055 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - 8s 318ms/step - loss: 2.3059 - accuracy: 0.1013 - sparse_categorical_crossentropy: 2.3031 - val_loss: 2.3054 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3055 - accuracy: 0.1025 - sparse_categorical_crossentropy: 2.3028 - val_loss: 2.3054 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3057 - accuracy: 0.0975 - sparse_categorical_crossentropy: 2.3032 - val_loss: 2.3053 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3050 - accuracy: 0.1050 - sparse_categorical_crossentropy: 2.3026 - val_loss: 2.3052 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - 8s 318ms/step - loss: 2.3051 - accuracy: 0.1013 - sparse_categorical_crossentropy: 2.3028 - val_loss: 2.3051 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3053 - accuracy: 0.0975 - sparse_categorical_crossentropy: 2.3031 - val_loss: 2.3049 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - 8s 313ms/step - loss: 2.3050 - accuracy: 0.0913 - sparse_categorical_crossentropy: 2.3029 - val_loss: 2.3047 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3048 - accuracy: 0.1063 - sparse_categorical_crossentropy: 2.3028 - val_loss: 2.3046 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3046 - accuracy: 0.1100 - sparse_categorical_crossentropy: 2.3027 - val_loss: 2.3046 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3049 - accuracy: 0.0900 - sparse_categorical_crossentropy: 2.3031 - val_loss: 2.3045 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3047 - accuracy: 0.0925 - sparse_categorical_crossentropy: 2.3030 - val_loss: 2.3045 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3044 - accuracy: 0.0900 - sparse_categorical_crossentropy: 2.3027 - val_loss: 2.3044 - val_accuracy: 0.0990 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3046 - accuracy: 0.0875 - sparse_categorical_crossentropy: 2.3030 - val_loss: 2.3043 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - 8s 318ms/step - loss: 2.3043 - accuracy: 0.0975 - sparse_categorical_crossentropy: 2.3027 - val_loss: 2.3041 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - 8s 312ms/step - loss: 2.3039 - accuracy: 0.1100 - sparse_categorical_crossentropy: 2.3024 - val_loss: 2.3041 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3041 - accuracy: 0.0925 - sparse_categorical_crossentropy: 2.3027 - val_loss: 2.3041 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3037 - accuracy: 0.1075 - sparse_categorical_crossentropy: 2.3024 - val_loss: 2.3041 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3040 - accuracy: 0.1013 - sparse_categorical_crossentropy: 2.3027 - val_loss: 2.3040 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3033 - accuracy: 0.1163 - sparse_categorical_crossentropy: 2.3021 - val_loss: 2.3040 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3038 - accuracy: 0.0875 - sparse_categorical_crossentropy: 2.3027 - val_loss: 2.3039 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3037 - accuracy: 0.0975 - sparse_categorical_crossentropy: 2.3026 - val_loss: 2.3039 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - 8s 320ms/step - loss: 2.3045 - accuracy: 0.0712 - sparse_categorical_crossentropy: 2.3034 - val_loss: 2.3038 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - 8s 318ms/step - loss: 2.3038 - accuracy: 0.0988 - sparse_categorical_crossentropy: 2.3028 - val_loss: 2.3038 - val_accuracy: 0.0972 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3033 - accuracy: 0.1013 - sparse_categorical_crossentropy: 2.3024 - val_loss: 2.3037 - val_accuracy: 0.0972 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3042 - accuracy: 0.0900 - sparse_categorical_crossentropy: 2.3032 - val_loss: 2.3036 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3038 - accuracy: 0.0988 - sparse_categorical_crossentropy: 2.3029 - val_loss: 2.3036 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - 8s 314ms/step - loss: 2.3038 - accuracy: 0.0862 - sparse_categorical_crossentropy: 2.3030 - val_loss: 2.3036 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - 8s 327ms/step - loss: 2.3035 - accuracy: 0.0862 - sparse_categorical_crossentropy: 2.3026 - val_loss: 2.3036 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - 8s 323ms/step - loss: 2.3031 - accuracy: 0.1213 - sparse_categorical_crossentropy: 2.3023 - val_loss: 2.3036 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3027 - accuracy: 0.0975 - sparse_categorical_crossentropy: 2.3020 - val_loss: 2.3036 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - 8s 326ms/step - loss: 2.3033 - accuracy: 0.1037 - sparse_categorical_crossentropy: 2.3025 - val_loss: 2.3036 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3040 - accuracy: 0.1037 - sparse_categorical_crossentropy: 2.3033 - val_loss: 2.3036 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3029 - accuracy: 0.1088 - sparse_categorical_crossentropy: 2.3022 - val_loss: 2.3035 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3037 - accuracy: 0.0950 - sparse_categorical_crossentropy: 2.3031 - val_loss: 2.3035 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3031 - accuracy: 0.0938 - sparse_categorical_crossentropy: 2.3025 - val_loss: 2.3035 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3031 - accuracy: 0.1138 - sparse_categorical_crossentropy: 2.3025 - val_loss: 2.3035 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - 8s 318ms/step - loss: 2.3030 - accuracy: 0.0925 - sparse_categorical_crossentropy: 2.3024 - val_loss: 2.3035 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3033 - accuracy: 0.1075 - sparse_categorical_crossentropy: 2.3027 - val_loss: 2.3035 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - 8s 313ms/step - loss: 2.3028 - accuracy: 0.1112 - sparse_categorical_crossentropy: 2.3023 - val_loss: 2.3035 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3030\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3037 - accuracy: 0.0850 - sparse_categorical_crossentropy: 2.3032 - val_loss: 2.3034 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3023 - accuracy: 0.1037 - sparse_categorical_crossentropy: 2.3018 - val_loss: 2.3034 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3026 - accuracy: 0.1125 - sparse_categorical_crossentropy: 2.3022 - val_loss: 2.3035 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3030\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3029 - accuracy: 0.1175 - sparse_categorical_crossentropy: 2.3025 - val_loss: 2.3035 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3031\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - 8s 313ms/step - loss: 2.3026 - accuracy: 0.1050 - sparse_categorical_crossentropy: 2.3022 - val_loss: 2.3035 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3031\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3033 - accuracy: 0.0975 - sparse_categorical_crossentropy: 2.3029 - val_loss: 2.3034 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3030\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - 8s 319ms/step - loss: 2.3026 - accuracy: 0.0913 - sparse_categorical_crossentropy: 2.3022 - val_loss: 2.3034 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3030\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3030 - accuracy: 0.0938 - sparse_categorical_crossentropy: 2.3026 - val_loss: 2.3033 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3030\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3032 - accuracy: 0.1000 - sparse_categorical_crossentropy: 2.3028 - val_loss: 2.3033 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3030\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3029 - accuracy: 0.1088 - sparse_categorical_crossentropy: 2.3026 - val_loss: 2.3033 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3030\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3037 - accuracy: 0.0838 - sparse_categorical_crossentropy: 2.3034 - val_loss: 2.3032 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3029\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - 8s 314ms/step - loss: 2.3032 - accuracy: 0.0925 - sparse_categorical_crossentropy: 2.3028 - val_loss: 2.3031 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3028\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3041 - accuracy: 0.0787 - sparse_categorical_crossentropy: 2.3038 - val_loss: 2.3030 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3027 - accuracy: 0.1063 - sparse_categorical_crossentropy: 2.3024 - val_loss: 2.3029 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - 8s 318ms/step - loss: 2.3024 - accuracy: 0.1037 - sparse_categorical_crossentropy: 2.3022 - val_loss: 2.3029 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3018 - accuracy: 0.1013 - sparse_categorical_crossentropy: 2.3015 - val_loss: 2.3029 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3028 - accuracy: 0.0962 - sparse_categorical_crossentropy: 2.3026 - val_loss: 2.3029 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - 8s 314ms/step - loss: 2.3034 - accuracy: 0.1000 - sparse_categorical_crossentropy: 2.3032 - val_loss: 2.3028 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3021 - accuracy: 0.0938 - sparse_categorical_crossentropy: 2.3019 - val_loss: 2.3028 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3025\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - 8s 318ms/step - loss: 2.3033 - accuracy: 0.0925 - sparse_categorical_crossentropy: 2.3031 - val_loss: 2.3028 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3025\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - 8s 314ms/step - loss: 2.3041 - accuracy: 0.1088 - sparse_categorical_crossentropy: 2.3039 - val_loss: 2.3028 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3034 - accuracy: 0.1025 - sparse_categorical_crossentropy: 2.3032 - val_loss: 2.3028 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - 8s 313ms/step - loss: 2.3029 - accuracy: 0.0975 - sparse_categorical_crossentropy: 2.3028 - val_loss: 2.3028 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3043 - accuracy: 0.0913 - sparse_categorical_crossentropy: 2.3041 - val_loss: 2.3028 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - 8s 314ms/step - loss: 2.3030 - accuracy: 0.0887 - sparse_categorical_crossentropy: 2.3028 - val_loss: 2.3029 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - 8s 315ms/step - loss: 2.3030 - accuracy: 0.1025 - sparse_categorical_crossentropy: 2.3028 - val_loss: 2.3029 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3031 - accuracy: 0.1013 - sparse_categorical_crossentropy: 2.3029 - val_loss: 2.3028 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - 8s 314ms/step - loss: 2.3028 - accuracy: 0.0875 - sparse_categorical_crossentropy: 2.3027 - val_loss: 2.3028 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - 8s 330ms/step - loss: 2.3032 - accuracy: 0.0887 - sparse_categorical_crossentropy: 2.3030 - val_loss: 2.3027 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - 8s 328ms/step - loss: 2.3033 - accuracy: 0.0850 - sparse_categorical_crossentropy: 2.3032 - val_loss: 2.3027 - val_accuracy: 0.1024 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3026 - accuracy: 0.1112 - sparse_categorical_crossentropy: 2.3025 - val_loss: 2.3028 - val_accuracy: 0.0938 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - 8s 318ms/step - loss: 2.3032 - accuracy: 0.0900 - sparse_categorical_crossentropy: 2.3030 - val_loss: 2.3028 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3027\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3029 - accuracy: 0.0938 - sparse_categorical_crossentropy: 2.3028 - val_loss: 2.3027 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - 8s 316ms/step - loss: 2.3029 - accuracy: 0.1112 - sparse_categorical_crossentropy: 2.3028 - val_loss: 2.3027 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3027 - accuracy: 0.1050 - sparse_categorical_crossentropy: 2.3026 - val_loss: 2.3027 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3026\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - 8s 317ms/step - loss: 2.3029 - accuracy: 0.1013 - sparse_categorical_crossentropy: 2.3028 - val_loss: 2.3026 - val_accuracy: 0.1007 - val_sparse_categorical_crossentropy: 2.3025\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}