{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Baseline_CNN_Music_Genre_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9QfKlnj8aC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install rarfile\n",
        "!pip install tensorflow==2.0.0-rc0\n",
        "!pip install scikit-image==0.15.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csZ4ZhEE4pHs",
        "colab_type": "code",
        "outputId": "353fe395-b470-47ae-ae75-c7c8ecc8e2bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path_data_set = '/content/drive/My Drive/dataset/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2O4EHPt5-gt",
        "colab_type": "text"
      },
      "source": [
        "# This script implements a simple baseline-CNN model for the Music Genre Classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoLuDFDg5-g1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# System/zip-handling imports\n",
        "import os, sys\n",
        "import zipfile\n",
        "import rarfile # Needs 'unrar'. On Ubuntu: install via \"sudo apt-get install unrar\"; pip didn't work here.\n",
        "\n",
        "# Imports tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Imports image handling\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage\n",
        "\n",
        "# For generating training and test data\n",
        "import random\n",
        "\n",
        "# Save training progress\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from shutil import copyfile  # Making copy of this file instance (including param settings used)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3ekx2Mt5-hF",
        "colab_type": "text"
      },
      "source": [
        "# IF DEALING WITH ZIP FILE!\n",
        "# Get sorted list of training file names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iFPE6s735-hH",
        "colab_type": "code",
        "outputId": "6400d6be-4f6a-4703-8c77-5da3f54db44b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Get access to zip-archive\n",
        "# Local:\n",
        "#archive = zipfile.ZipFile('../data/spectrograms.zip', 'r')\n",
        "#imgdata = archive.read('spectrograms/spectrogram_0000.png')\n",
        "\n",
        "# Colab:\n",
        "archive = zipfile.ZipFile('/content/drive/My Drive/ML/data/spectrograms.zip', 'r')\n",
        "imgdata = archive.read('spectrograms/spectrogram_0000.png')\n",
        "\n",
        "files = sorted([f for f in archive.namelist()[1:] if f.startswith('spectrograms/') and f.endswith('.png')])\n",
        "\n",
        "print(files)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3b4cd987a184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/ML/data/spectrograms.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimgdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spectrograms/spectrogram_0000.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spectrograms/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/ML/data/spectrograms.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtFv69DX5-hP",
        "colab_type": "text"
      },
      "source": [
        "# JFF: Get a feeling for the nature of the training and evaluation data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL5ZZx9X5-hS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Interpret image-data as image\n",
        "#image = cv2.imdecode(np.frombuffer(imgdata, dtype=np.uint8), 1)\n",
        "\n",
        "# Print some specs...\n",
        "#print('Type of image:')\n",
        "#print(type(image))\n",
        "#print('Dimension of a single image file:')\n",
        "#print(image.shape)\n",
        "\n",
        "# Show image visually (press any key in opening window in order to proceed in code...)\n",
        "#cv2.imshow(\"Image\", image)\n",
        "#cv2.waitKey(0)\n",
        "#cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Hhc7Bes1KwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W6QvaEJ5-hY",
        "colab_type": "text"
      },
      "source": [
        "# Read in both training and testing data from zip archive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVnTPkiR5-ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_set = []\n",
        "# Data storage\n",
        "combined_data = np.empty([1, 128, 128])\n",
        "\n",
        "# Read in images & store processed instances\n",
        "for f_name in files:\n",
        "    # Get image data from zip file\n",
        "    zip_img_data = archive.read(f_name)\n",
        "    image = cv2.imdecode(np.frombuffer(zip_img_data, dtype=np.uint8), 1)\n",
        "    \n",
        "    # Normalize image's colors to range [0, 1]\n",
        "    image = image / 255.0\n",
        "\n",
        "    #cv2.imshow(\"Normalized Image\", image)\n",
        "    #cv2.waitKey(0)\n",
        "\n",
        "    # Grayscale image\n",
        "    gray_image = skimage.color.rgb2gray(image)\n",
        "\n",
        "    #cv2.imshow(\"Grayscale Image\", gray_image)\n",
        "    #cv2.waitKey(0)\n",
        "    #cv2.destroyAllWindows()\n",
        "\n",
        "    # Store grayscaled image\n",
        "    combined_data = np.append(combined_data, [gray_image], axis=0)\n",
        "    \n",
        "# Remove initial, empty datapoint\n",
        "combined_data = combined_data[1:, :, :]\n",
        "\n",
        "print('Done reading in... Shape of data array:')\n",
        "print(combined_data.shape)\n",
        "print('Done.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOZRIlOS5-hg",
        "colab_type": "text"
      },
      "source": [
        "# Read in labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GFyuf9h5-hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_path = '../data/labels.txt'\n",
        "\n",
        "combined_labels = np.empty([1])\n",
        "\n",
        "with open(labels_path, 'r') as file:\n",
        "    for line in file:\n",
        "        combined_labels = np.append(combined_labels, [int(line)])\n",
        "\n",
        "# Remove initial, empty datapoint\n",
        "combined_labels = combined_labels[1:] \n",
        "print(combined_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHwdSTzZ5-ho",
        "colab_type": "text"
      },
      "source": [
        "# Divide data into train and test data:\n",
        "\n",
        "------------------------------------------------------\n",
        "Training data will be contained in:    training_data\n",
        "Tetsing  data will be contained in:    testing_data\n",
        "\n",
        "Training labels will be contained in:  training_labels\n",
        "Tetsing  labels will be contained in:  testing_labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEVZ0w1S5-hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get set of test-indices which indicates the training data points that have to be reserved for training\n",
        "percentage_test_data = 0.2\n",
        "population = range(len(combined_labels))\n",
        "nr_samples = int(percentage_test_data * len(combined_labels))\n",
        "\n",
        "test_indices = random.sample(population, nr_samples)\n",
        "test_indices = sorted(test_indices)\n",
        "\n",
        "print('Nr. test indices: ' + str(len(test_indices)))\n",
        "print('Test indices: ' + str(test_indices))\n",
        "\n",
        "\n",
        "# Split data into training- and test data, respectively - Preparation: Create empty arrays in which to later insert data\n",
        "test_len = len(test_indices)\n",
        "train_len = len(combined_labels)-len(test_indices)\n",
        "training_data, training_labels = np.empty([train_len, 128, 128]), np.empty([train_len])\n",
        "testing_data, testing_labels = np.empty([test_len, 128, 128]), np.empty([test_len])\n",
        "\n",
        "test_idx_list_idx = 0\n",
        "i = 0\n",
        "\n",
        "# Iterate through all data and assign each data point either to training data or testing data\n",
        "for data_idx in range(len(combined_labels)):\n",
        "\n",
        "    if test_idx_list_idx < nr_samples and data_idx == test_indices[test_idx_list_idx]:\n",
        "        testing_data[test_idx_list_idx, :, :] = combined_data[data_idx, :, :]\n",
        "        testing_labels[test_idx_list_idx] = combined_labels[data_idx]\n",
        "        test_idx_list_idx += 1\n",
        "        \n",
        "    else:\n",
        "        \n",
        "        training_data[i, :, :] = combined_data[data_idx, :, :]\n",
        "        training_labels[i] = combined_labels[data_idx]\n",
        "        i += 1\n",
        "        \n",
        "\n",
        "training_data = training_data.reshape([len(training_labels), 128, 128, 1])\n",
        "\n",
        "testing_data = testing_data.reshape([len(testing_labels), 128, 128, 1])\n",
        "        \n",
        "print('Final:')\n",
        "print(training_data.shape)\n",
        "print(training_labels.shape)\n",
        "print(testing_data.shape)\n",
        "print(testing_labels.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl1k85St5-hz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IF DEALING WITH RAR FILE!\n",
        "## Read in data & create training- and testing data sets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2vV9myL5-h4",
        "colab_type": "code",
        "outputId": "0f22d547-cf2f-48ba-c70d-937d69d5d7b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data_path = '/content/drive/My Drive/ML/data/'\n",
        "archive_name = 'spectograms_512.rar'\n",
        "\n",
        "if rarfile.is_rarfile(data_path + archive_name):\n",
        "    print('Is rar...')\n",
        "    # Read in file names per data (train vs test) set\n",
        "    class_labels = {'blues': 0,\n",
        "                    'classical': 1,\n",
        "                    'country': 2,\n",
        "                    'disco': 3,\n",
        "                    'hiphop': 4,\n",
        "                    'jazz': 5,\n",
        "                    'metal': 6,\n",
        "                    'pop': 7,\n",
        "                    'reggae': 8,\n",
        "                    'rock': 9\n",
        "                    }\n",
        "\n",
        "    rf = rarfile.RarFile(data_path + archive_name)\n",
        "\n",
        "    train_files, test_files = [], []\n",
        "    train_labels, test_labels = [], []\n",
        "\n",
        "    for f in rf.infolist():\n",
        "        f_name = f.filename\n",
        "        if f_name.startswith('spectrograms') and f_name.endswith('.png'):\n",
        "            p_parts = f_name.split('/')\n",
        "            label = class_labels[p_parts[1]]\n",
        "            if 'train' in f_name:\n",
        "                train_files.append(f_name)\n",
        "                train_labels.append(label)\n",
        "            elif 'test' in f.filename:\n",
        "                test_files.append(f_name)\n",
        "                test_labels.append(label)\n",
        "\n",
        "    num_training_data = len(train_labels)\n",
        "    num_testing_data = len(test_labels)\n",
        "    print('Len train data: ' + str(num_training_data))\n",
        "    print('Len test data: ' + str(num_testing_data))\n",
        "\n",
        "    # Read in data\n",
        "else:\n",
        "    print('No rar')\n",
        "    raise Exception('No rar!')\n",
        "\n",
        "##### TRAINING DATA AND LABELS #####\n",
        "\n",
        "# Get TRAIN image data from rar file\n",
        "dimension = np.array((cv2.imdecode(np.frombuffer(rf.read(train_files[0]), dtype=np.uint8), 1))).shape\n",
        "print('Dimensions: ' + str(dimension))\n",
        "print('Training set size: ' + str(num_training_data))\n",
        "\n",
        "training_data = np.empty([num_training_data, dimension[0], dimension[1]])\n",
        "print('Allocated.')\n",
        "\n",
        "for i, file in enumerate(train_files):\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print('Train: ' + str(i))\n",
        "\n",
        "    try:\n",
        "      rar_img_data = rf.read(file)\n",
        "      image = cv2.imdecode(np.frombuffer(rar_img_data, dtype=np.uint8), 1)\n",
        "\n",
        "      # Normalize image's colors to range [0, 1]\n",
        "      image = image / 255.0\n",
        "\n",
        "      # Grayscale image\n",
        "      gray_image = skimage.color.rgb2gray(image)\n",
        "\n",
        "      # Store grayscaled image\n",
        "      #training_data = np.append(training_data, [gray_image], axis=0)\n",
        "      training_data[i,:,:] = gray_image\n",
        "    except Exception as e:\n",
        "      print('Error: ' + str(file))\n",
        "      print(e)\n",
        "\n",
        "print('Done reading in... Shape of data array:')\n",
        "print(training_data.shape)\n",
        "print('Done.')\n",
        "\n",
        "# Get TRAIN labels\n",
        "training_labels = np.array(train_labels)\n",
        "print(train_labels)\n",
        "\n",
        "\n",
        "##### TESTING DATA AND LABELS #####\n",
        "\n",
        "# Get TEST image data from rar file\n",
        "testing_data = np.empty([num_testing_data, dimension[0], dimension[1]])  # TODO: change procedure here as well to pre-allocating space and NOT removing first element\n",
        "\n",
        "for i, file in enumerate(test_files):\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        print('Train: ' + str(i))\n",
        "    try:\n",
        "      rar_img_data = rf.read(file)\n",
        "      image = cv2.imdecode(np.frombuffer(rar_img_data, dtype=np.uint8), 1)\n",
        "\n",
        "      # Normalize image's colors to range [0, 1]\n",
        "      image = image / 255.0\n",
        "\n",
        "      # Grayscale image\n",
        "      gray_image = skimage.color.rgb2gray(image)\n",
        "\n",
        "      # Store grayscaled image\n",
        "      testing_data[i,:,:] = gray_image\n",
        "    except Exception as e:\n",
        "      print('Error: ' + str(file))\n",
        "      print(e)\n",
        "\n",
        "print('Done reading in... Shape of data array:')\n",
        "print(testing_data.shape)\n",
        "print('Done.')\n",
        "\n",
        "# Get TEST labels\n",
        "testing_labels = np.array(test_labels)\n",
        "print(testing_labels)\n",
        "rf = None # Clear ram space"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is rar...\n",
            "Len train data: 14400\n",
            "Len test data: 600\n",
            "Dimensions: (512, 512, 3)\n",
            "Training set size: 14400\n",
            "Allocated.\n",
            "Train: 0\n",
            "Train: 100\n",
            "Train: 200\n",
            "Train: 300\n",
            "Train: 400\n",
            "Train: 500\n",
            "Train: 600\n",
            "Train: 700\n",
            "Train: 800\n",
            "Train: 900\n",
            "Train: 1000\n",
            "Train: 1100\n",
            "Train: 1200\n",
            "Train: 1300\n",
            "Train: 1400\n",
            "Train: 1500\n",
            "Train: 1600\n",
            "Train: 1700\n",
            "Train: 1800\n",
            "Train: 1900\n",
            "Train: 2000\n",
            "Train: 2100\n",
            "Train: 2200\n",
            "Train: 2300\n",
            "Train: 2400\n",
            "Train: 2500\n",
            "Train: 2600\n",
            "Train: 2700\n",
            "Train: 2800\n",
            "Train: 2900\n",
            "Train: 3000\n",
            "Train: 3100\n",
            "Train: 3200\n",
            "Train: 3300\n",
            "Train: 3400\n",
            "Train: 3500\n",
            "Train: 3600\n",
            "Train: 3700\n",
            "Train: 3800\n",
            "Train: 3900\n",
            "Train: 4000\n",
            "Train: 4100\n",
            "Train: 4200\n",
            "Train: 4300\n",
            "Train: 4400\n",
            "Train: 4500\n",
            "Train: 4600\n",
            "Train: 4700\n",
            "Train: 4800\n",
            "Train: 4900\n",
            "Train: 5000\n",
            "Train: 5100\n",
            "Train: 5200\n",
            "Train: 5300\n",
            "Train: 5400\n",
            "Train: 5500\n",
            "Train: 5600\n",
            "Train: 5700\n",
            "Train: 5800\n",
            "Train: 5900\n",
            "Train: 6000\n",
            "Train: 6100\n",
            "Train: 6200\n",
            "Train: 6300\n",
            "Train: 6400\n",
            "Train: 6500\n",
            "Train: 6600\n",
            "Train: 6700\n",
            "Train: 6800\n",
            "Train: 6900\n",
            "Train: 7000\n",
            "Train: 7100\n",
            "Train: 7200\n",
            "Train: 7300\n",
            "Train: 7400\n",
            "Train: 7500\n",
            "Train: 7600\n",
            "Train: 7700\n",
            "Train: 7800\n",
            "Train: 7900\n",
            "Train: 8000\n",
            "Train: 8100\n",
            "Train: 8200\n",
            "Train: 8300\n",
            "Train: 8400\n",
            "Train: 8500\n",
            "Train: 8600\n",
            "Train: 8700\n",
            "Train: 8800\n",
            "Train: 8900\n",
            "Train: 9000\n",
            "Train: 9100\n",
            "Train: 9200\n",
            "Train: 9300\n",
            "Train: 9400\n",
            "Train: 9500\n",
            "Train: 9600\n",
            "Train: 9700\n",
            "Train: 9800\n",
            "Train: 9900\n",
            "Train: 10000\n",
            "Train: 10100\n",
            "Train: 10200\n",
            "Train: 10300\n",
            "Train: 10400\n",
            "Train: 10500\n",
            "Train: 10600\n",
            "Train: 10700\n",
            "Train: 10800\n",
            "Train: 10900\n",
            "Train: 11000\n",
            "Train: 11100\n",
            "Train: 11200\n",
            "Train: 11300\n",
            "Train: 11400\n",
            "Train: 11500\n",
            "Train: 11600\n",
            "Train: 11700\n",
            "Train: 11800\n",
            "Train: 11900\n",
            "Train: 12000\n",
            "Train: 12100\n",
            "Train: 12200\n",
            "Train: 12300\n",
            "Train: 12400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0wHsnS9xZtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
        "sess = tf.compat.v1.Session(config=config) \n",
        "\n",
        "tf.compat.v1.keras.backend.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH1zQ7c65-h8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set up folder for data gathering during training process"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP6g6QMR5-iC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up folder for data gathering during training process\n",
        "now = datetime.now()\n",
        "TIME_STAMP = now.strftime(\"_%Y_%d_%m__%H_%M_%S__%f\")\n",
        "MODEL_ID = 'Model_' + TIME_STAMP + '/'\n",
        "training_path = '../Trained_Models/CNN_Models/'\n",
        "path = training_path + MODEL_ID + '/'\n",
        "\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "else:\n",
        "    path = None\n",
        "    raise Exception('PATH EXISTS!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdqLFsMt5-iG",
        "colab_type": "text"
      },
      "source": [
        "# Set up the CNN architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHtomNP-5-iH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile $path/model_settings.text \n",
        "# Line above: Save model settings to file for reproducability - Run once with command above and once without. \n",
        "# First, it saves cell's content, but doesn't run the cell, afterwards, it's running the cell\n",
        "\n",
        "# Reset tf sessions\n",
        "tf.keras.backend.clear_session()  # Destroys the current TF graph and creates a new one.\n",
        "\n",
        "dimensions = 128\n",
        "classes = 10\n",
        "\n",
        "# Set up model architecture in terms of its layers\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(dimensions, dimensions, 1),\n",
        "                                                       kernel_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "model.add(layers.Dropout(0.05))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.05))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.05)))\n",
        "\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "model.add(layers.Dense(classes, activation='softmax'))\n",
        "\n",
        "# Note on regularizer(s), copied from https://www.tensorflow.org/tutorials/keras/overfit_and_underfit:\n",
        "# l2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value**2\n",
        "# to the total loss of the network.\n",
        "\n",
        "# Print summary\n",
        "model.summary()\n",
        "\n",
        "# Compile model & make some design choices\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001, #0.0\n",
        "                                           beta_1=0.9,\n",
        "                                           beta_2=0.999,\n",
        "                                           epsilon=1e-07,\n",
        "                                           amsgrad=False,\n",
        "                                           name='Adam'\n",
        "                                           ),\n",
        "              loss='sparse_categorical_crossentropy',  # Capable of working with regularization\n",
        "              metrics=['accuracy', 'sparse_categorical_crossentropy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZQBeh_G5-iN",
        "colab_type": "text"
      },
      "source": [
        "# Execute training:\n",
        "\n",
        "Start training process:\n",
        "    Run X times Y tensorflow-epochs and save a model as checkpoint after any Y epochs. \n",
        "    FIXME: Bit hacky solution, yet, but can be prettyfied.\n",
        "    \n",
        "IMPORTANT: 'accuracy'     == accuracy achieved during training on training data;  * the UN-important measure\n",
        "           'val_accuracy' == accuracy achieved on TEST data AFTER training epoch; * the important measure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkcIB61Q5-iO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definition of callbacks adjusted from https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping_callback = EarlyStopping(\n",
        "        monitor='val_accuracy',    # Stop training when `val_loss` is no longer improving\n",
        "        min_delta=0,               # \"no longer improving\" being defined as \"no better than 0|5e-1 less\"\n",
        "        patience=2,                # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
        "        verbose=0)                 # Quantity of printed output\n",
        "\n",
        "model_saving_callback = ModelCheckpoint(\n",
        "        filepath=path+'cnn_model.h5',\n",
        "        # Path where to save the model\n",
        "        # The two parameters below mean that we will overwrite\n",
        "        # the current checkpoint if and only if\n",
        "        # the `val_loss` score has improved.\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        # mode: one of {auto, min, max}. If `save_best_only=True`, the decision to\n",
        "        # overwrite the current save file is made based on either the maximization\n",
        "        # or the minimization of the monitored quantity. For `val_acc`, this\n",
        "        # should be `max`, for `val_loss` this should be `min`, etc. In `auto`\n",
        "        # mode, the direction is automatically inferred from the name of the\n",
        "        # monitored quantity.\n",
        "        verbose=0)\n",
        "\n",
        "# Join list of required callbacks\n",
        "callbacks = [model_saving_callback] #early_stopping_callback\n",
        "\n",
        "# Set number of desired epochs (mind the early-stopping!)\n",
        "epochs = 50\n",
        "\n",
        "# Perform x epochs of training\n",
        "history = model.fit(training_data, training_labels,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(testing_data, testing_labels),\n",
        "                    callbacks=callbacks, verbose=1)\n",
        "\n",
        "# Save the entire model as a final model to a HDF5 file.\n",
        "name = 'final_model'\n",
        "model.save(path+name+'.h5')\n",
        "\n",
        "# Record training progress\n",
        "with open(path+'training_progress.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"epoch\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\", \"sparse_categorical_crossentropy\"])\n",
        "    for line in range(len(history.history['loss'])): \n",
        "        epoch = str(line+1)\n",
        "        writer.writerow([epoch,\n",
        "                         history.history[\"loss\"][line], \n",
        "                         history.history[\"accuracy\"][line], \n",
        "                         history.history[\"val_loss\"][line], \n",
        "                         history.history[\"val_accuracy\"][line],\n",
        "                         history.history[\"sparse_categorical_crossentropy\"][line]\n",
        "                         ])\n",
        "    file.close()\n",
        "\n",
        "print('Done.')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}