{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "RNN_Music_Genre_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dLx_lpwQJVu",
        "colab_type": "text"
      },
      "source": [
        "# This script implements a simple baseline-RNN model for the Music Genre Classification task.\n",
        "It can be used as a Jupyter- or Colab-Notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "582qG_SHPuRN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Colab-Specific"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TuGXPA8P582",
        "colab_type": "text"
      },
      "source": [
        "## Mount GDrive to Colab session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csZ4ZhEE4pHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFXDtAAOPP2U",
        "colab_type": "text"
      },
      "source": [
        "##  Set path to dataset stored in GDrive\n",
        "Distinguish between train- and test data. Make sure dataset is added to '***My Drive***'! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPL9OSQ5PQp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_storage_path = '/content/drive/My Drive/ML/ML_experiments'\n",
        "path_train_data_set = '/content/drive/My Drive/ML/dataset_transformed/spectrograms512_train'\n",
        "path_test_data_set = '/content/drive/My Drive/ML/dataset_transformed/spectrograms512_test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFGMqYLsbtcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports mainly for debugging\n",
        "import IPython.display as display\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2O4EHPt5-gt",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh8g1UmcTxLV",
        "colab_type": "text"
      },
      "source": [
        "Specific tensorflow setup to make sure it's gonna run on GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrDMIlMSTwN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoLuDFDg5-g1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import os\n",
        "import os\n",
        "\n",
        "# Imports tensorflow\n",
        "from tensorflow.keras import datasets, layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import pathlib  # For data generators \n",
        "\n",
        "# Imports image handling\n",
        "import numpy as np\n",
        "\n",
        "# Save training progress\n",
        "import csv\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjCk6BALRQMp",
        "colab_type": "text"
      },
      "source": [
        "# Set up folder for data gathering during training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP6g6QMR5-iC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up folder for data gathering during training process\n",
        "now = datetime.now()\n",
        "TIME_STAMP = now.strftime(\"_%Y_%d_%m__%H_%M_%S__%f\")\n",
        "MODEL_ID = 'Model_' + TIME_STAMP\n",
        "training_path = data_storage_path + '/Trained_Models/RNN_Models/'\n",
        "path = training_path + MODEL_ID + '/'\n",
        "\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "    print('Created dir: ' + path)\n",
        "else:\n",
        "    path = None\n",
        "    raise Exception('PATH EXISTS!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K-HvGufSj0hk"
      },
      "source": [
        "# Set up the CNN architecture & helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yj77jEiWj0hZ"
      },
      "source": [
        "## Define Model Architecture\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3VMXaLivH9M",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLFfORtP5Ofv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "dimensions = 512  # Input dimension: 512x512\n",
        "units = 512       # Dimensionality of RNN output tensor\n",
        "classes = 10      # Number of output nodes in final layers (=nr of distinct classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgFTnykjI46T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Squeezing layer to transform (32, 512, 512, 1) to (32, 512, 512)\n",
        "\n",
        "class Squeezer(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(Squeezer, self).__init__()\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "  \n",
        "  def call(self, input):\n",
        "    return tf.squeeze(input, axis=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3_jWHCEWj0gz",
        "colab": {}
      },
      "source": [
        "# Reset tf sessions\n",
        "tf.keras.backend.clear_session()  # Destroys the current TF graph and creates a new one.\n",
        "\n",
        "\n",
        "############### Define RNN layers ###############\n",
        "\n",
        "# Full, efficient RNN implementation\n",
        "full_rnn_cell_1 = layers.SimpleRNN(units, # How many output units (per iteration of RNN)\n",
        "                                   input_shape=(1, dimensions), # 1 row of 512 pixels\n",
        "                                   kernel_regularizer=regularizers.l2(0.000001),\n",
        "                                   recurrent_regularizer=regularizers.l2(0.000001),\n",
        "                                   bias_regularizer=regularizers.l2(0.000001), \n",
        "                                   return_sequences=True  #return_sequences = X outputs per input image's row, here X = dim(image) = 512\n",
        "         ) \n",
        "full_rnn_cell_2 = layers.SimpleRNN(units, \n",
        "                                   input_shape=(1, dimensions),\n",
        "                                   kernel_regularizer=regularizers.l2(0.000001),\n",
        "                                   recurrent_regularizer=regularizers.l2(0.000001),\n",
        "                                   bias_regularizer=regularizers.l2(0.000001),\n",
        "                                   return_sequences=False\n",
        "         )\n",
        "#Alternatively: layers.SimpleRNN || layers.LSTM || layers.GRU\n",
        "\n",
        "\n",
        "\n",
        "############### Define model ###############\n",
        "\n",
        "# Set up model architecture in terms of its layers\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(Squeezer())\n",
        "\n",
        "# Add layers\n",
        "model.add(full_rnn_cell_1)\n",
        "model.add(full_rnn_cell_2)\n",
        "\n",
        "model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.000001)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(classes, activation='softmax'))\n",
        "\n",
        "# Note on regularizer(s), copied from https://www.tensorflow.org/tutorials/keras/overfit_and_underfit:\n",
        "# l2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value**2\n",
        "# to the total loss of the network.\n",
        "\n",
        "# Compile model & make some design choices\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001, #0.001\n",
        "                                           beta_1=0.9,\n",
        "                                           beta_2=0.999,\n",
        "                                           epsilon=1e-07,\n",
        "                                           amsgrad=False,\n",
        "                                           name='Adam'\n",
        "                                           ),\n",
        "              loss='sparse_categorical_crossentropy',  # Capable of working with regularization\n",
        "              metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
        "\n",
        "# Construct computational graph with proper dimensions\n",
        "inputs = np.random.random([32, 512, 512, 1]).astype(np.float32)\n",
        "model(inputs)\n",
        "\n",
        "# Print summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9GQK3EdQj0gx"
      },
      "source": [
        "## Define Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kzmDPmD-j0gp",
        "colab": {}
      },
      "source": [
        "# Definition of callbacks adjusted from https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "        monitor='val_accuracy',    # Stop training when `val_loss` is no longer improving\n",
        "        min_delta=0,               # \"no longer improving\" being defined as \"no better than 0|5e-1 less\"\n",
        "        patience=2,                # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
        "        verbose=0)                 # Quantity of printed output\n",
        "\n",
        "model_saving_callback = ModelCheckpoint(\n",
        "        filepath=path+'cnn_model.h5',\n",
        "        # Path where to save the model\n",
        "        # The two parameters below mean that we will overwrite\n",
        "        # the current checkpoint if and only if\n",
        "        # the `val_loss` score has improved.\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        # mode: one of {auto, min, max}. If `save_best_only=True`, the decision to\n",
        "        # overwrite the current save file is made based on either the maximization\n",
        "        # or the minimization of the monitored quantity. For `val_acc`, this\n",
        "        # should be `max`, for `val_loss` this should be `min`, etc. In `auto`\n",
        "        # mode, the direction is automatically inferred from the name of the\n",
        "        # monitored quantity.\n",
        "        verbose=0)\n",
        "\n",
        "# Join list of required callbacks\n",
        "callbacks = [model_saving_callback] # Outtake: early_stopping_callback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HIwZNgkSj0gn"
      },
      "source": [
        "## Define Data Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "toj4AoQPj0gj"
      },
      "source": [
        "**Pre**-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lb97V9CVj0ga",
        "colab": {}
      },
      "source": [
        "def preprocessing_function(x):\n",
        "    \"\"\"\n",
        "    Rotating image, such that time evolves over rows: 0'th row := 0'th time step, \n",
        "    512'th row := 512'th time step. Frequencies increasing from left image\n",
        "    border to right one. \n",
        "    \"\"\"\n",
        "    assert x.shape == (512, 512, 1)\n",
        "    # Rotate by 270° (=3*90°) \n",
        "    return np.rot90(x, 3)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k5HZC5l8j0gY"
      },
      "source": [
        "### Training data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v2IOGZ51j0gM",
        "colab": {}
      },
      "source": [
        "# The 1./255 is to convert from uint8 to float32 in range [0,1].\n",
        "train_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, \n",
        "                                                     preprocessing_function=preprocessing_function  # Pre-processing function may be passed here\n",
        "                                                     )\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_HEIGHT = 512\n",
        "IMG_WIDTH = 512\n",
        "STEPS_PER_EPOCH = 25\n",
        "data_dir = path_train_data_set\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "\n",
        "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
        "\n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(directory=str(data_dir),\n",
        "                                                     batch_size=BATCH_SIZE,\n",
        "                                                     color_mode='grayscale',  # Make sure that BW images are read in (indeed) as BW\n",
        "                                                     class_mode='sparse', # Class represented by 1 integer (instead of categorical==1-hot-encoding)\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     classes = list(CLASS_NAMES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cKnGPn62j0gG"
      },
      "source": [
        "### Test|Evaluation data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "efNAxPDlj0e4",
        "colab": {}
      },
      "source": [
        "# The 1./255 is to convert from uint8 to float32 in range [0,1].\n",
        "test_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, \n",
        "                                                     preprocessing_function=preprocessing_function\n",
        "                                                     )\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_HEIGHT = 512\n",
        "IMG_WIDTH = 512\n",
        "STEPS_PER_EPOCH = 25 #np.ceil(image_count/BATCH_SIZE)\n",
        "data_dir = path_test_data_set\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "\n",
        "\n",
        "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
        "\n",
        "\n",
        "test_data_gen = test_image_generator.flow_from_directory(directory=str(data_dir),\n",
        "                                                     batch_size=BATCH_SIZE,\n",
        "                                                     color_mode='grayscale',\n",
        "                                                     class_mode='sparse',\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     classes = list(CLASS_NAMES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fah_q4Reb5qe",
        "colab_type": "text"
      },
      "source": [
        "# Test data imports  (train) generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBY7H4l8cEZz",
        "colab_type": "text"
      },
      "source": [
        "### Helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSpuLmVwaV7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_batch(image_batch, label_batch):\n",
        "  print('Batch dimensions: ' + str(image_batch.shape))\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for n in range(25):\n",
        "      ax = plt.subplot(5,5,n+1)\n",
        "      image = image_batch[n]\n",
        "      # If grayscale, repeat 1 channel 3 times for visualization\n",
        "      if image.shape[2] == 1:\n",
        "        image = np.repeat(image, 3, axis=2)\n",
        "      plt.imshow(image)\n",
        "      plt.title(CLASS_NAMES[int(label_batch[n])].title())  # Doesn't work anymore after change of label encoding from 1-hot to spare\n",
        "      plt.axis('off')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmpqlR-ib_6n",
        "colab_type": "text"
      },
      "source": [
        "### Visualize generated example batch and labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n42lwOikbLjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_batch, label_batch = next(train_data_gen)\n",
        "show_batch(image_batch, label_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeOQv1XzSn3S",
        "colab_type": "text"
      },
      "source": [
        "# Perform training\n",
        "\n",
        "'accuracy' == accuracy achieved during training on training data\n",
        "\n",
        "'val_accuracy' == accuracy achieved on Test/Evaluation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkcIB61Q5-iO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert tf.config.list_physical_devices('GPU')\n",
        "assert tf.test.is_built_with_cuda()\n",
        "\n",
        "# Set number of desired epochs\n",
        "epochs = 100\n",
        "\n",
        "# Perform x epochs of training\n",
        "with tf.device('/device:GPU:0'):\n",
        "  history = model.fit(\n",
        "      x=train_data_gen,\n",
        "      #y=None,\n",
        "      #batch_size=None,\n",
        "      epochs=epochs,\n",
        "      verbose=1,\n",
        "      callbacks=callbacks,\n",
        "      #validation_split=0.0,\n",
        "      validation_data=test_data_gen,\n",
        "      shuffle=True,\n",
        "      #class_weight=None,\n",
        "      #sample_weight=None,\n",
        "      initial_epoch=0,\n",
        "      steps_per_epoch=25,\n",
        "      validation_steps=18,\n",
        "      #validation_freq=1,\n",
        "      max_queue_size=2,\n",
        "      #workers=1,\n",
        "      #use_multiprocessing=False,\n",
        "      #**kwargs\n",
        "  )\n",
        "\n",
        "# Save the entire model as a final model to a HDF5 file.\n",
        "name = 'final_model'\n",
        "model.save(path+name+'.h5')\n",
        "\n",
        "# Record training progress\n",
        "with open(path+'training_progress.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"epoch\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\", \"sparse_categorical_crossentropy\"])\n",
        "    for line in range(len(history.history['loss'])): \n",
        "        epoch = str(line+1)\n",
        "        writer.writerow([epoch,\n",
        "                         history.history[\"loss\"][line], \n",
        "                         history.history[\"accuracy\"][line], \n",
        "                         history.history[\"val_loss\"][line], \n",
        "                         history.history[\"val_accuracy\"][line], \n",
        "                         history.history[\"sparse_categorical_crossentropy\"][line]\n",
        "                         ])\n",
        "    # Save some more important bits/summary\n",
        "    writer.writerow([\"End of training. Summary:\"])\n",
        "    writer.writerow([\"epoch\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\", \"sparse_categorical_crossentropy\"])\n",
        "    # Max accuracy\n",
        "    writer.writerow([\"Max accuracy row:\"])\n",
        "    x = np.argmax(history.history[\"accuracy\"])\n",
        "    writer.writerow([str(x+1),\n",
        "                         history.history[\"loss\"][x], \n",
        "                         history.history[\"accuracy\"][x], \n",
        "                         history.history[\"val_loss\"][x], \n",
        "                         history.history[\"val_accuracy\"][x], \n",
        "                         history.history[\"sparse_categorical_crossentropy\"][x]\n",
        "                         ])\n",
        "    # Max val_accuracy\n",
        "    writer.writerow([\"Max val_accuracy row:\"])\n",
        "    x = np.argmax(history.history[\"val_accuracy\"])\n",
        "    writer.writerow([str(x+1),\n",
        "                         history.history[\"loss\"][x], \n",
        "                         history.history[\"accuracy\"][x], \n",
        "                         history.history[\"val_loss\"][x], \n",
        "                         history.history[\"val_accuracy\"][x], \n",
        "                         history.history[\"sparse_categorical_crossentropy\"][x]\n",
        "                         ])\n",
        "    file.close()\n",
        "\n",
        "print('Done.')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}